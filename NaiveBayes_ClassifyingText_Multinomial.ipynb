{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the news groups dataset from scikit-learn's datasets library\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the dataset\n",
    "news = fetch_20newsgroups()\n",
    "#this may take a few minutes as the dataset is large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\", \"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\"]\n"
     ]
    }
   ],
   "source": [
    "#printing the first 2 rows of the data\n",
    "print(news.data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "#printing the target names\n",
    "print(news.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 20 categories in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the target names in catgories variable\n",
    "categories = news.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the dataset to load ‘train’ for the training set, ‘test’ for the test set\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split between the train and test set is based upon a messages posted before and after a specific date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\n",
      "Subject: PB questions...\n",
      "Organization: Purdue University Engineering Computer Network\n",
      "Distribution: usa\n",
      "Lines: 36\n",
      "\n",
      "well folks, my mac plus finally gave up the ghost this weekend after\n",
      "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
      "new machine a bit sooner than i intended to be...\n",
      "\n",
      "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
      "of questions that (hopefully) somebody can answer:\n",
      "\n",
      "* does anybody know any dirt on when the next round of powerbook\n",
      "introductions are expected?  i'd heard the 185c was supposed to make an\n",
      "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
      "don't have access to macleak, i was wondering if anybody out there had\n",
      "more info...\n",
      "\n",
      "* has anybody heard rumors about price drops to the powerbook line like the\n",
      "ones the duo's just went through recently?\n",
      "\n",
      "* what's the impression of the display on the 180?  i could probably swing\n",
      "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
      "a feel for how much \"better\" the display is (yea, it looks great in the\n",
      "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
      "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
      "taking the disk size and money hit to get the active display?  (i realize\n",
      "this is a real subjective question, but i've only played around with the\n",
      "machines in a computer store breifly and figured the opinions of somebody\n",
      "who actually uses the machine daily might prove helpful).\n",
      "\n",
      "* how well does hellcats perform?  ;)\n",
      "\n",
      "thanks a bunch in advance for any info - if you could email, i'll post a\n",
      "summary (news reading time is at a premium with finals just around the\n",
      "corner... :( )\n",
      "--\n",
      "Tom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\n",
      "---------------------------------------------------------------------------\n",
      "\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\n",
      "Nietzsche\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printing the training data\n",
    "print(train.data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: mathew <mathew@mantis.co.uk>\n",
      "Subject: Re: STRONG & weak Atheism\n",
      "Organization: Mantis Consultants, Cambridge. UK.\n",
      "X-Newsreader: rusnews v1.02\n",
      "Lines: 9\n",
      "\n",
      "acooper@mac.cc.macalstr.edu (Turin Turambar, ME Department of Utter Misery) writes:\n",
      "> Did that FAQ ever got modified to re-define strong atheists as not those who\n",
      "> assert the nonexistence of God, but as those who assert that they BELIEVE in \n",
      "> the nonexistence of God?\n",
      "\n",
      "In a word, yes.\n",
      "\n",
      "\n",
      "mathew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printing the testing data\n",
    "print(test.data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this dataset for Machine Learning, we need to convert the content of each string into a vector of numbers. We can do this using TF-IDF vectorizer and create a pipeline that attaches it to a multinomial naive bayes classifier. \n",
    "\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency. It weighs the word counts by a measure of how often they appear in the documents instead of CountVectorizer which does the raw word count and puts too much weight on words that appear frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#importing Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#importing pipeline\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text classification, the features are related to word counts or frequencies within the documents to be classified. We have used Multinomial Naive Bayes as it is appropriate for features that represent counts or count rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use pipleine to attach MultinomialNB and TfidfVectorizer\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB(alpha=0.1))\n",
    "\n",
    "# here alpha is the Laplace smoothning parameter (which helps in achieving results even if the word has not \n",
    "# appeared in the training data, that is, adding a value to the obsereved count) and by altering the values \n",
    "# of alpha, we got to know that 0.1 gives us the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('multinomialnb',\n",
       "                 MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying the model to the training data\n",
    "model.fit(train.data, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 11  0 ...  9  3 15]\n"
     ]
    }
   ],
   "source": [
    "#predicting labels for the test data\n",
    "pred_labels = model.predict(test.data)\n",
    "print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81388938732554\n"
     ]
    }
   ],
   "source": [
    "#f1 score of the model\n",
    "from sklearn import metrics\n",
    "\n",
    "#since default average is binary and our problem is multinomial so we have chosen macro in our case as it \n",
    "#will compute the metric independently for each class and then take the average (hence treating all classes equally).\n",
    "\n",
    "print(metrics.f1_score(test.target, pred_labels, average='macro'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# con_mat = confusion_matrix(test.target, pred_labels)\n",
    "\n",
    "# heatmap of confusion matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.figure(figsize=(20,20))\n",
    "# ax = sns.heatmap(con_mat.T, annot=True, fmt='.1g', xticklabels=train.target_names, yticklabels=train.target_names)\n",
    "# plt.xlabel('True labels')\n",
    "# plt.ylabel('Predicted labels')\n",
    "\n",
    "# bottom, top = ax.get_ylim()\n",
    "# ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "# plt.show()\n",
    "\n",
    "#we dropped the idea of creating confusion matrix due to large number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us define a function to predict the label/category of any sentence\n",
    "def predict_label(s, train=train, model=model):\n",
    "    pred = model.predict([s])\n",
    "    return train.target_names[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.autos'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label('Elon Musk is working on new electric car model')\n",
    "#correct prediction by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.sys.mac.hardware'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label('I love my Mac')\n",
    "#correct prediction by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talk.politics.mideast'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label('Narendra Modi is helping the country fight against coronavirus')\n",
    "#correct prediction by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the classification of text documents using sparse features (like TfidfVectorizer) shuffles the training and test data, instead of segmenting by time. And in that case, multinomial Naive Bayes gives such a higher F-score of 0.88. \n",
    "\n",
    "Let us try to find out if the model is really performing that well or is overfitting. We can know this by removing the header, footer and quotes, which are common in all news groups and fitting the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train', remove = ('header','footer','quotes'), categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', remove = ('header','footer','quotes'), categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB(alpha=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('multinomialnb',\n",
       "                 MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train.data, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = model.predict(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765567386988352\n"
     ]
    }
   ],
   "source": [
    "print(metrics.f1_score(pred_labels, test.target, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our F1 score reduced a lot, from 81% to 76%, after removing headers, footers and quotes. But, this is more realistic than our earlier score as over model is not overfitting now. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
